{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow with 3 convnet layers for 2 classes using all slices of x-plane\n",
    "2 classes, 2 subject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have achieved 100% on training set of 2 classes for only x=20 images of 3D fMRI. This is an attempt to train 10 slices of x-plane, [15-25]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import nibabel as nib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "dataset and the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 1 and data label shapes are (40, 64, 64, 1452) (1452,)\n"
     ]
    }
   ],
   "source": [
    "data_s1 = nib.load('/home/farahana/Documents/dataset/Haxby2001/subj1/bold.nii')\n",
    "data_s2 = nib.load('/home/farahana/Documents/dataset/Haxby2001/subj2/bold.nii')\n",
    "\n",
    "label_s1 = np.recfromcsv('/home/farahana/Documents/dataset/Haxby2001/subj1/labels.txt', delimiter=' ')\n",
    "session = label_s1['chunks']\n",
    "y = label_s1['labels']\n",
    "\n",
    "print(\"Subject 1 and data label shapes are\", data_s1.shape,label_s1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the 'rest' and 'house' labels\n",
    "shoe_state = y == b'shoe'\n",
    "house_state = y == b'house'\n",
    "\n",
    "# Divide the rest and house state for labels and data\n",
    "y_shoe = session[shoe_state]\n",
    "y_house = session[house_state]\n",
    "\n",
    "y_shoe_one = np.zeros(y_shoe.shape[0], dtype = object)\n",
    "y_shoe_one[:] = 0\n",
    "\n",
    "y_house_one = np.zeros(y_house.shape[0], dtype = object)\n",
    "y_house_one[:] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting subject using get_data() of nibabel\n",
    "data_1 = data_s1.get_data()\n",
    "data_2 = data_s2.get_data()\n",
    "\n",
    "idx = data_s1.shape[0] # take the shape:40 as idx\n",
    "\n",
    "data_r1 = np.reshape(data_1[[0],:,:,[y_shoe,y_house]],(64*64,-1)).T\n",
    "data_r2 = np.reshape(data_2[[0],:,:,[y_shoe,y_house]],(64*64,-1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 1 and Subject 2 reshape size are (8640, 4096) (8640, 4096)\n"
     ]
    }
   ],
   "source": [
    "# Reshaping the data to 40 slices of x-plane\n",
    "for i in range(1,idx):\n",
    "    data_reshape_1 = np.reshape(data_1[i,:,:,[y_shoe,y_house]],(64*64,-1)).T\n",
    "    data_r1 = np.append(data_r1[:], data_reshape_1[:], axis = 0)\n",
    "\n",
    "    \n",
    "for i in range(1,idx):\n",
    "    data_reshape_2 = np.reshape(data_2[i,:,:,[y_shoe,y_house]],(64*64,-1)).T\n",
    "    data_r2 = np.append(data_r2[:], data_reshape_2[:], axis = 0)\n",
    "\n",
    "print(\"Subject 1 and Subject 2 reshape size are\", data_r1.shape,data_r2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17280, 4096)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.concatenate((data_r1[:], data_r2[:]), axis = 0)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data session and data label shape (17280, 2) (17280,)\n"
     ]
    }
   ],
   "source": [
    "# Appending label data to associate with every 40 slices of x-plane of data\n",
    "label_session = np.concatenate((y_shoe_one[:], y_house_one[:]), axis=0)\n",
    "label_1 = np.tile(label_session, idx)\n",
    "\n",
    "data_session = (np.arange(2) == label_1[:, None]).astype(np.float32)\n",
    "label_data = np.concatenate((label_1, label_1),axis=0)\n",
    "data_session = np.concatenate((data_session, data_session),axis=0)\n",
    "\n",
    "print (\"data session and data label shape\", data_session.shape, label_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data session and data label shape (17280, 2) (17280,)\n"
     ]
    }
   ],
   "source": [
    "data_session = (np.arange(2) == label_data[:, None]).astype(np.float32)\n",
    "print (\"data session and data label shape\", data_session.shape, label_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Splitting data (using sklearn)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, data_session, test_size = 0.2, random_state = 123)\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(data, label_data, test_size = 0.2, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of train and test set:  (13824, 4096) (3456, 4096)\n"
     ]
    }
   ],
   "source": [
    "print (\"size of train and test set: \", X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conventional Layers (with TensorFlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables and initialization\n",
    "\n",
    "img_size = 64 # height/width pixel image\n",
    "img_size_flat = img_size*img_size\n",
    "img_shape = (img_size, img_size)\n",
    "num_classes = 2\n",
    "\n",
    "# Convolutional Layer 1\n",
    "filter_size1 = 5\n",
    "num_filters1 = 52\n",
    "\n",
    "# Convolutional Layer 2\n",
    "filter_size2 = 5\n",
    "num_filters2 = 128\n",
    "\n",
    "# Convolutional Layer \n",
    "filter_size3 = 5\n",
    "num_filters3 = 200\n",
    "\n",
    "# Fully-connected layer\n",
    "fc_size = 400\n",
    "\n",
    "num_channels = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, cls_true, cls_pred=None):\n",
    "    #assert len(images) == len(cls_true) == 11\n",
    "    \n",
    "    # Create figure with 3 by 3 subplots\n",
    "    fig, axes = plt.subplots(4,3)\n",
    "    fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "    \n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(images[i].reshape(img_shape),cmap=plt.cm.gray)\n",
    "        \n",
    "        if cls_pred is None:\n",
    "            xlabel = \"True: {0}\".format(cls_true[i])\n",
    "        else:\n",
    "            xlabel = \"True: {0}, Pred:  {1}\".format(cls_true[i], cls_pred[i])\n",
    "            \n",
    "        ax.set_xlabel(xlabel)\n",
    "        \n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Placeholder\n",
    "x = tf.placeholder(tf.float32, shape=[None, img_size_flat], name='x')\n",
    "x_image = tf.reshape(x, [-1, img_size, img_size, num_channels])\n",
    "\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, 2], name='y_true')\n",
    "y_true_cls = tf.argmax(y_true, dimension=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Variables\n",
    "\n",
    "# 2.1 Helper-functions for creating new variables\n",
    "def new_weights(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.05))\n",
    "    \n",
    "def new_biases(length):\n",
    "    return tf.Variable(tf.constant(0.05, shape = [length]))\n",
    "\n",
    "# 2.2 Helper-function for creating new Convolutional Layer\n",
    "# Input is a 4-dimension:[image #, y-axis, x-axis, channel of each image]\n",
    "# Ouput is a 4-dimension:[image #, y_axis, x-axis, channell produced by conv-layer]\n",
    "# for Y-axis and x-axis of output, if 2x2 pooling is used, the h and w is divided by 2\n",
    "def new_conv_layer (input,              # the previous layer\n",
    "                   num_input_channels,  # num channels of previous layer\n",
    "                   filter_size,         # h and w of each filter\n",
    "                   num_filters,\n",
    "                   use_pooling = True):  # use 2x2 max-pooling\n",
    "    shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "    \n",
    "    weights = new_weights(shape=shape)\n",
    "    biases = new_biases(length=num_filters)\n",
    "    \n",
    "    # creating TF operation for conv\n",
    "    layer = tf.nn.conv2d(input=input, \n",
    "                         filter=weights, \n",
    "                         strides=[1,1,1,1], # the strides are set to 1 in all dimension; strides=[image #, 1, 1, input_channel]\n",
    "                         padding='SAME')    # iput image is padded with zeros so the size of output is the same\n",
    "    \n",
    "    # add the biases to the result of convolution, layer = layer + biases\n",
    "    layer += biases\n",
    "    \n",
    "    # pooling is used to donwsample\n",
    "    if use_pooling:\n",
    "        layer = tf.nn.max_pool(value=layer,\n",
    "                              ksize=[1,2,2,1],\n",
    "                              strides=[1,2,2,1],\n",
    "                              padding='SAME')\n",
    "\n",
    "    # rectified linear unit (Relu)=max(x,0) \n",
    "    # will add some non-linearity to allow complicated functions learning\n",
    "    layer = tf.nn.relu(layer)\n",
    "    \n",
    "    # side notes:\n",
    "    # normally relu is before max-pooling, but,\n",
    "    # since relu(max_pool(x)) == max_pool(relu(x)), 75% of relu-operation is saved\n",
    "    \n",
    "    return layer, weights\n",
    "\n",
    "# 2.3 Helper-function for layer flattening\n",
    "# from 4-dimension convolutional layer ouput to 2-dimension fully-connected layer input\n",
    "def flatten_layer(layer):\n",
    "    layer_shape = layer.get_shape()\n",
    "    \n",
    "    # layer_shape = [num_images, img_w, img_h, num_channels]\n",
    "    # number of features = img_w*img_h*num_channels\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "    \n",
    "    # Reshape the layer to [num_images, num_features]\n",
    "    # pass '[-1]' to flatten or '-1' to infer the shape\n",
    "    # now, the layer_flat will be [num_images, img_h*img_w*num_channels]\n",
    "    layer_flat = tf.reshape(layer, [-1, num_features])\n",
    "    \n",
    "    return layer_flat, num_features\n",
    "\n",
    "# 2.4 Helper-function for creating the Fully-connected layer\n",
    "# receive from the flatten-layer\n",
    "def new_fc_layer(input,\n",
    "                num_inputs,\n",
    "                num_outputs,\n",
    "                use_relu=True):\n",
    "    \n",
    "    # create new weights and biases\n",
    "    weights = new_weights(shape=[num_inputs, num_outputs])\n",
    "    biases = new_biases(length=num_outputs)\n",
    "    \n",
    "    layer = tf.matmul(input, weights) + biases\n",
    "    \n",
    "    # if relu is true,\n",
    "    if use_relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    "        \n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Mathematical models\n",
    "\n",
    "# 3.1 convolutional layer 1\n",
    "layer_conv1, weights_conv1 = new_conv_layer(input=x_image,\n",
    "                                           num_input_channels=num_channels,\n",
    "                                           filter_size = filter_size1,\n",
    "                                           num_filters = num_filters1,\n",
    "                                           use_pooling = True)\n",
    "\n",
    "# 3.2 convolutional layer 2\n",
    "layer_conv2, weights_conv2 = new_conv_layer(input=layer_conv1,\n",
    "                                           num_input_channels=num_filters1,\n",
    "                                           filter_size = filter_size2,\n",
    "                                           num_filters = num_filters2,\n",
    "                                           use_pooling = True)\n",
    "\n",
    "# 3.3 convolutional layer \n",
    "layer_conv3, weights_conv3 = new_conv_layer(input=layer_conv2,\n",
    "                                           num_input_channels=num_filters2,\n",
    "                                           filter_size = filter_size3,\n",
    "                                           num_filters = num_filters3,\n",
    "                                           use_pooling = True)\n",
    "\n",
    "# 3.2 Flatten layer\n",
    "layer_flat, num_features = flatten_layer(layer_conv3)\n",
    "\n",
    "# 3.5.1 Fully-connected layer 1\n",
    "layer_fc1 = new_fc_layer(input=layer_flat,\n",
    "                        num_inputs=num_features,\n",
    "                        num_outputs = fc_size,\n",
    "                        use_relu=True)\n",
    "\n",
    "# 3.5.. Dropout layer\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "layer_fc1_drop = tf.nn.dropout(layer_fc1, keep_prob)\n",
    "\n",
    "# 3.5.2 Fully-connected layer 2\n",
    "layer_fc2 = new_fc_layer(input=layer_fc1_drop,\n",
    "                        num_inputs=fc_size,\n",
    "                        num_outputs = num_classes,\n",
    "                        use_relu=False)\n",
    "\n",
    "# nomalize the outputs(sum of each row=1, each element is in range [0.1])\n",
    "y_pred = tf.nn.softmax(layer_fc2)\n",
    "\n",
    "y_pred_cls = tf.argmax(y_pred, dimension=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Cost measurement\n",
    "## With this measurement, the model is expected to perform better with help of y_pred and y_true analysis\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits (logits=layer_fc2, labels=y_true)\n",
    "\n",
    "# get the single scalar value\n",
    "cost = tf.reduce_mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Optimization\n",
    "## use advanced form of gradient descent which is adamoptimizer \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Performance Measure\n",
    "correct_pred = tf.equal(y_pred_cls, y_true_cls)\n",
    "\n",
    "## calculate the accuracy by number of correctly corrected/number of input numbers\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create session\n",
    "session = tf.Session()\n",
    "\n",
    "### Initiate using CPU, by stating GPU:0\n",
    "#config = tf.ConfigProto(\n",
    "#        device_count = {'GPU': 0}\n",
    "#    )\n",
    "#session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Initialize Variables\n",
    "session.run(tf.initialize_all_variables())\n",
    "\n",
    "# 2.1 Helper functions to get a random training-batch\n",
    "batch_size = 20\n",
    "\n",
    "def random_batch():\n",
    "    # Number of images (transfer-values) in the training-set.\n",
    "    num_images = len(X_train)\n",
    "\n",
    "    # Create a random index.\n",
    "    idx = np.random.choice(num_images,\n",
    "                           size=batch_size,\n",
    "                           replace=False)\n",
    "\n",
    "    # Use the random index to select random x and y-values.\n",
    "    # We use the transfer-values instead of images as x-values.\n",
    "    x_batch = X_train[idx]\n",
    "    y_batch = y_train[idx]\n",
    "\n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Use helper-function to get batch calculation for optimization\n",
    "\n",
    "total_iterations = 0\n",
    "\n",
    "def optimize(num_iterations):\n",
    "    # to update the global variable rather a local copy\n",
    "    global total_iterations\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i in range(total_iterations, total_iterations+num_iterations):\n",
    "        # Get a batch of training examples\n",
    "        x_batch, y_true_batch = random_batch()\n",
    "        # put the batch in a dictionary with proper names (the placeholders)\n",
    "        feed_dict_train = { x: x_batch, \n",
    "                           y_true: y_true_batch, keep_prob:1.0}\n",
    "        \n",
    "        # Run the optimizer using the batch training data\n",
    "        # How? Tf will assign the variables in feed_dict_train to the placeholder and run the optimization\n",
    "        session.run(optimizer, feed_dict = feed_dict_train)\n",
    "        \n",
    "        # to print status of every 100 iterations\n",
    "        if i%100 == 0:\n",
    "            acc = session.run(accuracy, feed_dict=feed_dict_train)\n",
    "            \n",
    "            msg = \"Optimization Iterations: {0:>6}, Training Accuracy: {1:>6.1%}\"\n",
    "            \n",
    "            print(msg.format(i+1, acc))\n",
    "            \n",
    "    total_iterations += num_iterations\n",
    "        \n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    print(\"Time usage: \"+ str(timedelta(seconds=int(round(time_dif)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define the performance measures functions for show\n",
    "def plot_example_errors(cls_pred, correct):    \n",
    "    incorrect = (correct == False)\n",
    "    \n",
    "    images_incorrect = data.test.images[incorrect]\n",
    "    cls_pred_incorrect = cls_pred[incorrect]\n",
    "    cls_true_incorrect = data.test.cls[incorrect]\n",
    "    \n",
    "    plot_images (images =images_incorrect[0:1], \n",
    "                 cls_true = cls_true_incorrect[0:1], \n",
    "                 cls_pred = cls_pred_incorrect[0:1])\n",
    "\n",
    "def plot_confusion_matrix(cls_pred):\n",
    "    cls_true = data.test.cls\n",
    "\n",
    "    cm = confusion_matrix(y_true=cls_true, y_pred=cls_pred)\n",
    "    print(cm)\n",
    "    \n",
    "    #Plot the cm as image\n",
    "    plt.matshow(cm)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(num_classes)\n",
    "    plt.xticks(tick_marks, range(num_classes))\n",
    "    plt.yticks(tick_marks, range(num_classes))\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True Class')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the test-set into smaller batches of this size.\n",
    "test_batch_size = 30 \n",
    "\n",
    "def print_test_accuracy(show_example_errors=False,\n",
    "                       show_confusion_matrix=False):\n",
    "    num_test = len(X_test)\n",
    "    \n",
    "    # array allocation that is the output of batches\n",
    "    cls_pred = np.zeros(shape=num_test, dtype=np.int)\n",
    "    \n",
    "    i = 0\n",
    "    while i<num_test:\n",
    "        j = min(i+test_batch_size, num_test) # end-indexed  for next batch\n",
    "        images = X_test[i:j, :] # images between i and j\n",
    "        labels = y_test[i:j, :] \n",
    "        \n",
    "        feed_dict = {x:images, y_true: labels, keep_prob: 0.5}\n",
    "        \n",
    "        cls_pred[i:j] = session.run(y_pred_cls, feed_dict=feed_dict)\n",
    "        i = j\n",
    "    \n",
    "    cls_true = y_test1\n",
    "    \n",
    "    correct = (cls_true == cls_pred)\n",
    "    correct_sum = correct.sum()\n",
    "    \n",
    "    acc = float(correct_sum) / num_test\n",
    "    \n",
    "    # Print the accuracy.\n",
    "    msg = \"Accuracy on Test-Set: {0:.1%} ({1} / {2})\"\n",
    "    print(msg.format(acc, correct_sum, num_test))\n",
    "\n",
    "    # Plot some examples of mis-classifications, if desired.\n",
    "    if show_example_errors:\n",
    "        print(\"Example errors:\")\n",
    "        plot_example_errors(cls_pred=cls_pred, correct=correct)\n",
    "\n",
    "    # Plot the confusion matrix, if desired.\n",
    "    if show_confusion_matrix:\n",
    "        print(\"Confusion Matrix:\")\n",
    "        plot_confusion_matrix(cls_pred=cls_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test-Set: 52.6% (1818 / 3456)\n"
     ]
    }
   ],
   "source": [
    "print_test_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Iterations:      1, Training Accuracy:  55.0%\n",
      "Time usage: 0:00:00\n"
     ]
    }
   ],
   "source": [
    "# First Optimization\n",
    "optimize(num_iterations=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test-Set: 48.0% (1659 / 3456)\n"
     ]
    }
   ],
   "source": [
    "print_test_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time usage: 0:00:01\n"
     ]
    }
   ],
   "source": [
    "# 100th Optimization\n",
    "optimize(num_iterations=99) # We already performed 1 iteration above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test-Set: 53.5% (1848 / 3456)\n"
     ]
    }
   ],
   "source": [
    "print_test_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Iterations:    101, Training Accuracy:  55.0%\n",
      "Optimization Iterations:    201, Training Accuracy:  70.0%\n",
      "Optimization Iterations:    301, Training Accuracy:  85.0%\n",
      "Optimization Iterations:    401, Training Accuracy:  80.0%\n",
      "Optimization Iterations:    501, Training Accuracy:  60.0%\n",
      "Optimization Iterations:    601, Training Accuracy:  85.0%\n",
      "Optimization Iterations:    701, Training Accuracy:  80.0%\n",
      "Optimization Iterations:    801, Training Accuracy:  80.0%\n",
      "Optimization Iterations:    901, Training Accuracy:  90.0%\n",
      "Time usage: 0:00:13\n"
     ]
    }
   ],
   "source": [
    "# 1000th Optimization\n",
    "optimize(num_iterations=900) # We already performed 100 iteration above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test-Set: 59.8% (2068 / 3456)\n"
     ]
    }
   ],
   "source": [
    "print_test_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Iterations:   1001, Training Accuracy: 100.0%\n",
      "Optimization Iterations:   1101, Training Accuracy:  85.0%\n",
      "Optimization Iterations:   1201, Training Accuracy:  80.0%\n",
      "Optimization Iterations:   1301, Training Accuracy:  95.0%\n",
      "Optimization Iterations:   1401, Training Accuracy:  95.0%\n",
      "Optimization Iterations:   1501, Training Accuracy:  85.0%\n",
      "Optimization Iterations:   1601, Training Accuracy:  90.0%\n",
      "Optimization Iterations:   1701, Training Accuracy:  90.0%\n",
      "Optimization Iterations:   1801, Training Accuracy:  90.0%\n",
      "Optimization Iterations:   1901, Training Accuracy:  95.0%\n",
      "Time usage: 0:00:14\n"
     ]
    }
   ],
   "source": [
    "optimize(num_iterations=1000) # We already performed 910 iteration above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test-Set: 64.2% (2219 / 3456)\n"
     ]
    }
   ],
   "source": [
    "print_test_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
